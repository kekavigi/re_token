{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "KATEGLO = pd.read_csv('data/kateglo.csv')[['lema', 'nilai', 'kelas']].fillna('')\n",
    "DICT = {k: v for _,k,v in KATEGLO[['lema', 'kelas']].to_records()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: https://stackoverflow.com/questions/42742810/speed-up-millions\n",
    "# -of-regex-replacements-in-python-3/42789508#42789508\n",
    "\n",
    "#KATA[KATA.lema.apply(lambda x: '.' in str(x))] ==> empty\n",
    "with open('data/singkatan.txt', 'r') as f:\n",
    "    ABBREV = set(f.read().split('\\n')) - {''}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://news.detik.com/berita-jawa-tengah/d-5589430/kondisi-terkini-eks-pimpinan-yang-tersengal-sengal-di-gedung-dprd-klaten\n",
    "TEXT = '''Klaten - Pimpinan DPRD Klaten periode 2005-2010, Anang Widayaka dievakuasi dari gedung DPRD Klaten dalam kondisi napas tersengal-sengal kemarin. Saat ini kondisi Anang disebut sudah membaik.\n",
    "\"Tadi malam saya mendapat kabar terakhir dari keluarga, sekitar pukul 20.30 WIB kondisinya membaik. Alat bantu oksigen sudah dilepas,\" ungkap Wakil Sekretaris Bidang Organisasi, Kaderisasi dan Keanggotaan DPD Partai Golkar Klaten, Purwanto saat dihubungi detikcom, Selasa (1/6/2021).\n",
    "\n",
    "Purwanto menuturkan dari riwayat kesehatan, yang bersangkutan memang selama ini rutin kontrol ke RS di Yogyakarta. Dalam sepekan Anang dua kali kontrol dokter.\n",
    "\n",
    "\"Beliau (Anang) itu biasanya Selasa dan Jumat kontrol ke RS di Yogyakarta. Tapi sakitnya apa tidak tahu,\" lanjut Purwanto.\n",
    "\n",
    "Sejak dua tahun terakhir, jelas Purwanto, Anang pernah juga dirawat di RS. Saat melihat Anang berkunjung ke gedung DPRD dirinya juga kaget.\n",
    "\n",
    "\"Pas di DPRD saya juga kaget. Saya pikir mungkin sudah sehat,\" imbuh Purwanto.\n",
    "\n",
    "Purwanto menuturkan kemarin Anang ke Gedung DPRD Klaten hanya ditemani satu orang. Dia menduga kondisi Anang karena sudah mendekati jadwal kontrol.\n",
    "\n",
    "\"Ke DPRD hanya sama satu temannya. Informasinya karena mendekati jadwal kontrol kadang kondisinya menurun, jadi sudah bawa alat bantu dan tabung oksigen di mobil,\" ucap Purwanto yang sempat ikut mengevakuasi Anang.\n",
    "\n",
    "Anang, ucap Purwanto, merupakan politikus senior dari Partai Golkar. Dia pun mengaku banyak mendapat telepon untuk menanyakan kabar kondisi Anang.\n",
    "\n",
    "\"Saya ditelepon banyak orang, dari provinsi dan pusat menanyakan kondisi terakhir. Beliau (Anang) termasuk salah satu guru politik saya,\" lanjut Purwanto.\n",
    "\n",
    "Sebelumnya diberitakan, mantan Wakil Ketua DPRD Kabupaten Klaten, Anang Widayaka, dievakuasi dari Gedung DPRD Klaten, Senin (31/5) kemarin. Eks pimpinan DPRD dari Partai Golkar itu saat dievakuasi karena kondisinya yang tersengal-sengal saat berada di ruang Ketua DPRD Klaten Hamenang Wajar Ismoyo.\n",
    "\n",
    "Pantauan detikcom di DPRD Klaten, politikus senior itu dievakuasi dari lantai dua, Senin (31/5) sekitar pukul 13.00 WIB. Saat dievakuasi dengan lift, Anang tampak dalam kondisi sadar tapi napasnya terlihat berat dan sudah dibantu tabung oksigen.\n",
    "\n",
    "Anang dibawa dari ruang dinas Ketua DPRD Klaten, Hamenang Wajar Ismoyo, menggunakan kursi beroda. Dia langsung dibawa oleh mobil ambulans Pemkab Klaten menuju RSUP Dr Soeradji Tirtonegoro.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RE Word$_T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.geeksforgeeks.org/python-check-url-string/\n",
    "# dimodifikasi agar hanya menampilkan group pertama\n",
    "# TODO: Watchout, ada kasus yang menyebabkan regex lama dieksekusi\n",
    "# DeprecationWarning: Flags not at the start of the expression '^((?:(?i)\\\\b(?:(?:htt'\n",
    "URLS = r\"(?i)\\b(?:(?:https?:\\/\\/|www\\d{0,3}[.]|[a-z0-9.\\-]+\" +\\\n",
    "    r\"[.][a-z]{2,4}\"\"\\/)(?:[^\\s()<>]+|\\((?:[^\\s()<>]+|(?:\\(\" +\\\n",
    "    r\"?:[^\\s()<>]+\\)))*\\))+(?:\\((?:[^\\s()<>]+|(?:\\([^\\s()<>\" +\\\n",
    "    r\"] +\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "\n",
    "EMAIL = r'[\\w.+-]+@[\\w-]+\\.(?:[\\w-]\\.?)+[\\w-]'\n",
    "\n",
    "# Format jam umum, III.A3.\n",
    "TIME = r'\\d{2}\\.\\d{2}\\.\\d{2}'\n",
    "\n",
    "# Format tanda hubung: III.E2., III.E3., III.E4., III.E5. III.E6.\n",
    "HYPHEN = r'(?:[a-zA-Z]+(?:-[a-zA-Z]+)+)'\n",
    "#AFFIX = KATEGLO[KATEGLO.kelas=='bt'].lema.to_list()\n",
    "#AFFIX = r'\\b(?:%s)' % r'|'.join(AFFIX)\n",
    "\n",
    "# Format bilangan ribuan, III.A5.\n",
    "# ditambah kemungkinan desimal, cth: 213.126,56\n",
    "# kemungkinan simbol positif dan negatif tidak diurus\n",
    "# ditambah bentuk umum \\d+\n",
    "NUMBER = r'(?:\\d{4,}|(?:\\d{1,3}(?:\\.\\d{3})*))(?:\\,\\d*)?'\n",
    "\n",
    "# urus kemungkinan kasus II.H3., II.H4. II.H1.\n",
    "ABBREV = r'\\b(?:%s)' % r'|'.join(ABBREV)\n",
    "ABBREV = ABBREV.replace(r'.', r'\\.')\n",
    "NAME_ABBR = r'\\b[A-Z]\\.'\n",
    "\n",
    "# Format III.G2., III.I1., III.I2.\n",
    "#   simbol -- sebagai subtitusi tanda pisah\n",
    "#   simbol-simbol lainnya sebagai token\n",
    "#   TODO: respect word boundaries\n",
    "SYMBOLS = r'(?:%s)' % r'|'.join(r'''\\(\\?\\) \\.{3} -- - \\n \\. \\, ; : \\! \" ' \\? \\( \\) [ ] / —'''.split(' '))\n",
    "\n",
    "#karakter non-spasi yang tersisa sebagai token\n",
    "WORD = r'(?:[a-zA-Z]+)'\n",
    "UNKNOWN = r'(?:\\S+)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexes = [EMAIL, TIME, HYPHEN, NUMBER, ABBREV,\n",
    "           NAME_ABBR, SYMBOLS, WORD, UNKNOWN]\n",
    "PATTERN = re.compile(r'(%s)' % '|'.join(regexes), re.VERBOSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_tokenize(text: str, verbose=True, del_newline=True):\n",
    "    text = re.sub(r'(“|”|\")', r'\"', text)\n",
    "    text = re.sub(r'\\(\\s*\\?\\s*\\)', '(?)', text)\n",
    "    if del_newline: text = text.replace('\\n', ' ')\n",
    "\n",
    "    result =  PATTERN.findall(text)\n",
    "    if not verbose: return result\n",
    "\n",
    "    tmp = []\n",
    "    for token in result:\n",
    "        if re.fullmatch(EMAIL, token):\n",
    "            tmp.append((token, 'OBJ', 'URL'))\n",
    "            continue\n",
    "        if re.fullmatch(TIME, token):\n",
    "            tmp.append((token, 'OBJ', 'TIME'))\n",
    "            continue\n",
    "        if re.fullmatch(HYPHEN, token):\n",
    "            tmp.append((token, 'WORD', ''))\n",
    "            continue\n",
    "        #if re.fullmatch(AFFIX, token):\n",
    "        #    tmp.append((token, 'WORD', ''))\n",
    "        #    continue\n",
    "        if re.fullmatch(NUMBER, token):\n",
    "            tmp.append((token, 'OBJ', 'NUM'))\n",
    "            continue\n",
    "        if re.fullmatch(EMAIL, token):\n",
    "            tmp.append((token, 'OBJ', 'URL'))\n",
    "            continue\n",
    "        if re.fullmatch(ABBREV, token):\n",
    "            tmp.append((token, 'WORD', ''))\n",
    "            continue\n",
    "        if re.fullmatch(NAME_ABBR, token):\n",
    "            tmp.append((token, 'WORD', ''))\n",
    "            continue\n",
    "        if re.fullmatch(SYMBOLS, token):\n",
    "            tmp.append((token, 'SYMB', ''))\n",
    "            continue\n",
    "        if re.fullmatch(WORD, token):\n",
    "            tmp.append((token, 'WORD', ''))\n",
    "            continue            \n",
    "        tmp.append((token, 'OBJ', ''))\n",
    "    \n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "422"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = regex_tokenize(TEXT)\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pasca-panen', 'WORD', '')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex_tokenize('pasca-panen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AFFIX' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-b1469225e346>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mAFFIX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'AFFIX' is not defined"
     ]
    }
   ],
   "source": [
    "AFFIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correcting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sastrawi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = sastrawi.stemming.Stemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.stem('menggunakan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ngram spell-checking\n",
    "\n",
    "Maybe just use `import enchant`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORDLIST = KATEGLO.lema[KATEGLO.lema.apply(lambda x: not(' ' in x) )].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram(text, n=3):\n",
    "    if len(text)<n: text+=' '*(n-len(text))\n",
    "    return set(text[i:i+n] for i in range(len(text)-n+1))\n",
    "\n",
    "ngram('menggunakan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "NGRAM = dict()\n",
    "\n",
    "for word in WORDLIST:\n",
    "    for ng in ngram(word):\n",
    "        if ng not in NGRAM:\n",
    "            NGRAM[ng] = set()\n",
    "        NGRAM[ng].add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plausible(word):\n",
    "    score = {}\n",
    "    \n",
    "    for ng in ngram(word):\n",
    "        if ng not in NGRAM:\n",
    "            continue\n",
    "        for word in NGRAM[ng]:\n",
    "            if word not in score:\n",
    "                score[word] = 0\n",
    "            score[word] += 1\n",
    "    iterable = iter((val, word) for word, val in score.items())\n",
    "    return sorted(iterable, reverse=True)[:10]\n",
    "    \n",
    "def suggest(word):\n",
    "    # choose if there is multiple max value...\n",
    "    max_ =  plausible(word)\n",
    "    if max_: max_ = max_[0]\n",
    "    else: return word\n",
    "\n",
    "    if plausible(max_[1])[0][0] - max_[0] <= 2:\n",
    "        return max_[1]\n",
    "    return word\n",
    "\n",
    "plausible('Selasar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in tokens:\n",
    "    if token[1]!='WORD' or token[0].isupper():\n",
    "        #print(token[0], end=' ')\n",
    "        continue\n",
    "\n",
    "    ist = token[0].istitle()\n",
    "    mask = token[0].lower()\n",
    "\n",
    "    if mask in WORDLIST:\n",
    "        #print(token[0], end=' ')\n",
    "        continue\n",
    "    \n",
    "    sugg = suggest(token[0])\n",
    "    if ist: sugg = sugg.title()\n",
    "\n",
    "    if sugg==token[0]:\n",
    "        #print(token[0], end=' ')\n",
    "        continue\n",
    "\n",
    "    print('{} : {}'.format(token[0], sugg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_resolve(tokens: list):\n",
    "    tmp = tokens.copy()\n",
    "    result = []\n",
    "    \n",
    "    while tmp:\n",
    "        token, type_ = tmp.pop(0)\n",
    "                        \n",
    "        new_token = token\n",
    "        # underestimate, trt to expand word\n",
    "        while tmp and tmp[0][1]=='WORD':\n",
    "            new_token += ' ' + tmp[0][0]\n",
    "            if new_token not in DICT: break\n",
    "            token = new_token\n",
    "            tmp.pop(0)\n",
    "\n",
    "        result.append((token, type_))\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_resolve(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'makan-makan' in DICT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Daftar Isi",
   "title_sidebar": "Daftar Isi",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
